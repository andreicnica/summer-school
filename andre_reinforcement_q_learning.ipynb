{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code adapted from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) tutorial\n",
    "=====================================\n",
    "**Author**: `Adam Paszke <https://github.com/apaszke>`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gym env\n",
    "# env = gym.make('CartPole-v0').unwrapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gathering envrionment\n",
    "\n",
    "from gathering_mae.single_agent_wrapper import SingleAgentGatheringEnv\n",
    "from configs.utils import load_config\n",
    "import cv2\n",
    "\n",
    "# Get default config\n",
    "\n",
    "# cfg = load_config(\"configs/static_simple.yaml\")\n",
    "\n",
    "cfg = load_config(\"configs/default_env.yaml\")\n",
    "\n",
    "env = SingleAgentGatheringEnv(cfg)\n",
    "obs_size: torch.Size = env.observation_space\n",
    "no_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN algorithm\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_size[0], 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.head = nn.Linear(288, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu((self.conv1(x)))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(MLP, self).__init__()\n",
    "        in_units = reduce(mul, in_size, 1)\n",
    "        hidden_size = 256\n",
    "        self.ln1 = nn.Linear(in_units, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn1(self.ln1(x)))\n",
    "        x = F.relu(self.bn2(self.ln2(x)))\n",
    "        return self.head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input extraction & processing\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize env capture\n",
    "def convert_obs_to_screen(obs):\n",
    "    obs = obs.clone()\n",
    "    obs.mul_(255).int()\n",
    "    obs = obs.numpy().astype(np.uint8)\n",
    "    return obs\n",
    "\n",
    "def process_obs(obs):\n",
    "    obs = obs.unsqueeze(0).unsqueeze(0)\n",
    "    obs.add_(-0.6588499999999999).mul_(2.)\n",
    "    return obs\n",
    "\n",
    "obs = env.reset()\n",
    "screen = env.render()\n",
    "view_obs = convert_obs_to_screen(obs)\n",
    "\n",
    "# Plot full map + partial \n",
    "plt.figure()\n",
    "plt.imshow(screen,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of game full map & agent observation')\n",
    "plt.show()\n",
    "\n",
    "# Plot actual observation \n",
    "plt.figure()\n",
    "plt.imshow(view_obs,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of agent actual received observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation & visualization \n",
    "\n",
    "def eval_agent(env, model, no_episodes = 5, show = True, waitkey=0):\n",
    "\n",
    "    model.eval()\n",
    "    done = True\n",
    "    env_step = 0\n",
    "    \n",
    "    eval_returns = []\n",
    "    eval_lengths = []\n",
    "    for ep in range(no_episodes):\n",
    "        ep_return = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        env.render(imshow=show)\n",
    "\n",
    "        for t in count():                \n",
    "\n",
    "            with torch.no_grad():\n",
    "                state = process_obs(state).to(device)\n",
    "                action = model(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "            if show:\n",
    "                key = cv2.waitKey(waitkey) & 0xFF\n",
    "\n",
    "                # if the 'ESC' key is pressed, Quit\n",
    "                if key == 27:\n",
    "                    show = False\n",
    "\n",
    "            state, r, done, _ = env.step(action)\n",
    "            ep_return += r\n",
    "\n",
    "            env.render(imshow=show)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eval_returns.append(ep_return)\n",
    "        eval_lengths = [t + 1]\n",
    "        \n",
    "    print(f\"[Eval {i_episode:d}]  Return: {np.mean(eval_returns):5.2f}\",\n",
    "      \" | Ep. length:\", np.mean(eval_lengths))\n",
    "\n",
    "\n",
    "# Report info\n",
    "wait_key = False\n",
    "print_freq = 1\n",
    "\n",
    "\n",
    "screen = env.render()\n",
    "cv2.namedWindow(\"Waikey\")\n",
    "wait_key = True\n",
    "play_game_freq = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 4\n",
    "EP_START_OPTIM = 2\n",
    "\n",
    "policy_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "target_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "returns = []\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(no_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "^^^^^^^^^^^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    policy_net.train()\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).to(device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "eval_no_episodes = 5\n",
    "ep_return = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train 0] Return: -1.00 | Ep. length: 100.0\n",
      "[Eval 0]  Return: -10.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 1] Return:  8.50 | Ep. length: 100.0\n",
      "[Eval 1]  Return: 10.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 2] Return:  6.33 | Ep. length: 100.0\n",
      "[Eval 2]  Return:  9.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 3] Return: 11.00 | Ep. length: 100.0\n",
      "[Eval 3]  Return:  2.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 4] Return: 10.00 | Ep. length: 100.0\n",
      "[Eval 4]  Return: 31.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 5] Return: 11.00 | Ep. length: 100.0\n",
      "[Eval 5]  Return: 54.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 6] Return:  9.57 | Ep. length: 100.0\n",
      "[Eval 6]  Return: 19.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 7] Return:  9.00 | Ep. length: 100.0\n",
      "[Eval 7]  Return: 58.40  | Ep. length: 100.0\n",
      "\n",
      "[Eval 7]  Return: 51.00  | Ep. length: 100.0\n",
      "[Train 8] Return: 10.67 | Ep. length: 100.0\n",
      "[Eval 8]  Return: 15.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 9] Return: 10.50 | Ep. length: 100.0\n",
      "[Eval 9]  Return: 23.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 10] Return:  9.73 | Ep. length: 100.0\n",
      "[Eval 10]  Return: 37.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 11] Return:  7.33 | Ep. length: 100.0\n",
      "[Eval 11]  Return: 41.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 12] Return:  7.38 | Ep. length: 100.0\n",
      "[Eval 12]  Return: 60.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 13] Return:  7.14 | Ep. length: 100.0\n",
      "[Eval 13]  Return: 40.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 14] Return:  6.47 | Ep. length: 100.0\n",
      "[Eval 14]  Return: 50.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 15] Return:  7.69 | Ep. length: 100.0\n",
      "[Eval 15]  Return: 21.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 16] Return:  7.59 | Ep. length: 100.0\n",
      "[Eval 16]  Return: 45.80  | Ep. length: 100.0\n",
      "\n",
      "[Eval 16]  Return: 47.00  | Ep. length: 100.0\n",
      "[Train 17] Return:  7.11 | Ep. length: 100.0\n",
      "[Eval 17]  Return: 32.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 18] Return:  7.11 | Ep. length: 100.0\n",
      "[Eval 18]  Return: 31.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 19] Return:  6.35 | Ep. length: 100.0\n",
      "[Eval 19]  Return: 50.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 20] Return:  6.14 | Ep. length: 100.0\n",
      "[Eval 20]  Return: 45.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 21] Return:  6.27 | Ep. length: 100.0\n",
      "[Eval 21]  Return: 40.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 22] Return:  6.30 | Ep. length: 100.0\n",
      "[Eval 22]  Return: 40.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 23] Return:  6.00 | Ep. length: 100.0\n",
      "[Eval 23]  Return: 30.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 24] Return:  6.24 | Ep. length: 100.0\n",
      "[Eval 24]  Return: 51.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 25] Return:  6.85 | Ep. length: 100.0\n",
      "[Eval 25]  Return: 30.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 26] Return:  6.63 | Ep. length: 100.0\n",
      "[Eval 26]  Return: 30.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 27] Return:  7.43 | Ep. length: 100.0\n",
      "[Eval 27]  Return: 30.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 28] Return:  8.03 | Ep. length: 100.0\n",
      "[Eval 28]  Return: 11.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 29] Return:  7.87 | Ep. length: 100.0\n",
      "[Eval 29]  Return: 45.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 30] Return:  7.52 | Ep. length: 100.0\n",
      "[Eval 30]  Return: 69.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 31] Return:  7.25 | Ep. length: 100.0\n",
      "[Eval 31]  Return: 21.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 32] Return:  7.52 | Ep. length: 100.0\n",
      "[Eval 32]  Return: 41.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 33] Return:  6.85 | Ep. length: 100.0\n",
      "[Eval 33]  Return: 11.00  | Ep. length: 100.0\n",
      "\n",
      "[Eval 33]  Return: 98.00  | Ep. length: 100.0\n",
      "[Train 34] Return:  6.63 | Ep. length: 100.0\n",
      "[Eval 34]  Return: 50.20  | Ep. length: 100.0\n",
      "\n",
      "[Eval 34]  Return: 49.00  | Ep. length: 100.0\n",
      "[Train 35] Return:  6.31 | Ep. length: 100.0\n",
      "[Eval 35]  Return:  1.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 36] Return:  7.05 | Ep. length: 100.0\n",
      "[Eval 36]  Return: 65.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 37] Return:  7.32 | Ep. length: 100.0\n",
      "[Eval 37]  Return: 51.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 38] Return:  7.18 | Ep. length: 100.0\n",
      "[Eval 38]  Return: 45.20  | Ep. length: 100.0\n",
      "\n",
      "[Eval 38]  Return: 98.00  | Ep. length: 100.0\n",
      "[Train 39] Return:  7.70 | Ep. length: 100.0\n",
      "[Eval 39]  Return: 40.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 40] Return:  7.80 | Ep. length: 100.0\n",
      "[Eval 40]  Return: 40.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 41] Return:  8.10 | Ep. length: 100.0\n",
      "[Eval 41]  Return: 60.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 42] Return:  8.19 | Ep. length: 100.0\n",
      "[Eval 42]  Return: 30.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 43] Return:  8.43 | Ep. length: 100.0\n",
      "[Eval 43]  Return: 50.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 44] Return:  8.29 | Ep. length: 100.0\n",
      "[Eval 44]  Return: 30.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 45] Return:  8.43 | Ep. length: 100.0\n",
      "[Eval 45]  Return: 40.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 46] Return:  8.68 | Ep. length: 100.0\n",
      "[Eval 46]  Return: 26.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 47] Return:  8.58 | Ep. length: 100.0\n",
      "[Eval 47]  Return: 22.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 48] Return:  8.80 | Ep. length: 100.0\n",
      "[Eval 48]  Return: 50.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 49] Return:  8.80 | Ep. length: 100.0\n",
      "[Eval 49]  Return: 11.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 50] Return:  9.06 | Ep. length: 100.0\n",
      "[Eval 50]  Return: 30.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 51] Return:  9.13 | Ep. length: 100.0\n",
      "[Eval 51]  Return:  4.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 52] Return:  9.15 | Ep. length: 100.0\n",
      "[Eval 52]  Return: 13.20  | Ep. length: 100.0\n",
      "\n",
      "[Eval 52]  Return:  9.00  | Ep. length: 100.0\n",
      "[Train 53] Return:  9.56 | Ep. length: 100.0\n",
      "[Eval 53]  Return: 35.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 54] Return:  9.93 | Ep. length: 100.0\n",
      "[Eval 54]  Return: 21.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 55] Return:  9.88 | Ep. length: 100.0\n",
      "[Eval 55]  Return: 19.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 56] Return: 10.16 | Ep. length: 100.0\n",
      "[Eval 56]  Return: 32.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 57] Return: 10.12 | Ep. length: 100.0\n",
      "[Eval 57]  Return: 28.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 58] Return: 10.08 | Ep. length: 100.0\n",
      "[Eval 58]  Return: 30.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 59] Return: 10.30 | Ep. length: 100.0\n",
      "[Eval 59]  Return: 26.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 60] Return: 10.20 | Ep. length: 100.0\n",
      "[Eval 60]  Return: 37.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 61] Return: 10.50 | Ep. length: 100.0\n",
      "[Eval 61]  Return: 38.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 62] Return: 10.46 | Ep. length: 100.0\n",
      "[Eval 62]  Return: 38.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 63] Return: 10.45 | Ep. length: 100.0\n",
      "[Eval 63]  Return:  6.40  | Ep. length: 100.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-19444c813e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mEP_START_OPTIM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-61b61a788b01>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# columns of actions taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0dfa610a4d12>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#screen = env.render()\n",
    "#cv.named\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = process_obs(state)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state.to(device))\n",
    "        action = action.cpu()\n",
    "        current_screen, reward, done, _ = env.step(action.item())\n",
    "        current_screen = process_obs(current_screen)\n",
    "\n",
    "        ep_return += reward\n",
    "\n",
    "        \n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = current_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward.cpu())\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if i_episode > EP_START_OPTIM:\n",
    "            optimize_model()\n",
    "\n",
    "        if done:\n",
    "            returns.append(ep_return)\n",
    "            episode_durations.append(t + 1)\n",
    "            ep_return = 0\n",
    "            \n",
    "            if i_episode % print_freq == 0:\n",
    "                print(f\"[Train {i_episode:d}] Return: {np.mean(returns):5.2f} \"\n",
    "                      f\"| Ep. length: {np.mean(episode_durations)}\")\n",
    "                eval_agent(env, policy_net, no_episodes = eval_no_episodes, show = False)\n",
    "                print()\n",
    "                \n",
    "            if wait_key and i_episode % play_game_freq == 0:\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == 27:\n",
    "                    cv2.destroyAllWindows()\n",
    "                elif key == 113:  # Null action q\n",
    "                    eval_agent(env, policy_net, no_episodes = 1, show = True)\n",
    "\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
