{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code adapted from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) tutorial\n",
    "=====================================\n",
    "**Author**: `Adam Paszke <https://github.com/apaszke>`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gym env\n",
    "# env = gym.make('CartPole-v0').unwrapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gathering envrionment\n",
    "\n",
    "from gathering_mae.single_agent_wrapper import SingleAgentGatheringEnv\n",
    "from configs.utils import load_config\n",
    "import cv2\n",
    "\n",
    "# Get default config\n",
    "\n",
    "# cfg = load_config(\"configs/static_simple.yaml\")\n",
    "\n",
    "cfg = load_config(\"configs/default_env.yaml\")\n",
    "\n",
    "env = SingleAgentGatheringEnv(cfg)\n",
    "obs_size: torch.Size = env.observation_space\n",
    "no_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN algorithm\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_size[0], 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.head = nn.Linear(288, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu((self.conv1(x)))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(MLP, self).__init__()\n",
    "        in_units = reduce(mul, in_size, 1)\n",
    "        hidden_size = 256\n",
    "        self.ln1 = nn.Linear(in_units, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn1(self.ln1(x)))\n",
    "        x = F.relu(self.bn2(self.ln2(x)))\n",
    "        return self.head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input extraction & processing\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize env capture\n",
    "def convert_obs_to_screen(obs):\n",
    "    obs = obs.clone()\n",
    "    obs.mul_(255).int()\n",
    "    obs = obs.numpy().astype(np.uint8)\n",
    "    return obs\n",
    "\n",
    "def process_obs(obs):\n",
    "    obs = obs.unsqueeze(0).unsqueeze(0)\n",
    "    obs.add_(-0.6588499999999999).mul_(2.)\n",
    "    return obs\n",
    "\n",
    "obs = env.reset()\n",
    "screen = env.render()\n",
    "view_obs = convert_obs_to_screen(obs)\n",
    "\n",
    "# Plot full map + partial \n",
    "plt.figure()\n",
    "plt.imshow(screen,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of game full map & agent observation')\n",
    "plt.show()\n",
    "\n",
    "# Plot actual observation \n",
    "plt.figure()\n",
    "plt.imshow(view_obs,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of agent actual received observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation & visualization \n",
    "\n",
    "def eval_agent(env, model, no_episodes = 5, show = True, waitkey=0):\n",
    "\n",
    "    model.eval()\n",
    "    done = True\n",
    "    env_step = 0\n",
    "    \n",
    "    eval_returns = []\n",
    "    eval_lengths = []\n",
    "    for ep in range(no_episodes):\n",
    "        ep_return = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        env.render(imshow=show)\n",
    "\n",
    "        for t in count():                \n",
    "\n",
    "            with torch.no_grad():\n",
    "                state = process_obs(state).to(device)\n",
    "                action = model(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "            if show:\n",
    "                key = cv2.waitKey(waitkey) & 0xFF\n",
    "\n",
    "                # if the 'ESC' key is pressed, Quit\n",
    "                if key == 27:\n",
    "                    show = False\n",
    "\n",
    "            state, r, done, _ = env.step(action)\n",
    "            ep_return += r\n",
    "\n",
    "            env.render(imshow=show)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eval_returns.append(ep_return)\n",
    "        eval_lengths = [t + 1]\n",
    "        \n",
    "    print(f\"[Eval {i_episode:d}]  Return: {np.mean(eval_returns):5.2f}\",\n",
    "      \" | Ep. length:\", np.mean(eval_lengths))\n",
    "\n",
    "\n",
    "# Report info\n",
    "wait_key = False\n",
    "print_freq = 1\n",
    "\n",
    "\n",
    "# screen = env.render()\n",
    "# cv2.namedWindow(\"Waikey\")\n",
    "# wait_key = True\n",
    "# play_game_freq = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.4\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 4\n",
    "EP_START_OPTIM = 2\n",
    "OPTIMIZE_FREQ = 4\n",
    "\n",
    "policy_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "target_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "steps_done = 0\n",
    "steps_cnt = 0\n",
    "episode_durations = []\n",
    "returns = []\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if steps_done % (EPS_DECAY // 4) == 0:\n",
    "        print(f\"\\-- EPS: {eps_threshold}\")\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(no_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "^^^^^^^^^^^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    policy_net.train()\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).to(device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "eval_no_episodes = 5\n",
    "ep_return = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train 0] Return: -12.00 | Ep. length: 100.0\n",
      "[Eval 0]  Return:  0.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 1] Return:  0.00 | Ep. length: 100.0\n",
      "[Eval 1]  Return: -10.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 2] Return:  1.67 | Ep. length: 100.0\n",
      "[Eval 2]  Return: -6.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 3] Return:  2.25 | Ep. length: 100.0\n",
      "[Eval 3]  Return: -0.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 4] Return:  2.40 | Ep. length: 100.0\n",
      "[Eval 4]  Return: -0.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 5] Return:  3.17 | Ep. length: 100.0\n",
      "[Eval 5]  Return:  2.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 6] Return:  3.86 | Ep. length: 100.0\n",
      "[Eval 6]  Return: -0.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 7] Return:  3.62 | Ep. length: 100.0\n",
      "[Eval 7]  Return:  9.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 8] Return:  3.00 | Ep. length: 100.0\n",
      "[Eval 8]  Return: 11.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 9] Return:  2.90 | Ep. length: 100.0\n",
      "[Eval 9]  Return: 29.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 10] Return:  2.82 | Ep. length: 100.0\n",
      "[Eval 10]  Return: 35.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 11] Return:  3.75 | Ep. length: 100.0\n",
      "[Eval 11]  Return: 32.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 12] Return:  5.00 | Ep. length: 100.0\n",
      "[Eval 12]  Return: 15.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 13] Return:  5.36 | Ep. length: 100.0\n",
      "[Eval 13]  Return: 19.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 14] Return:  5.13 | Ep. length: 100.0\n",
      "[Eval 14]  Return: 18.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 15] Return:  5.31 | Ep. length: 100.0\n",
      "[Eval 15]  Return: 39.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 16] Return:  5.53 | Ep. length: 100.0\n",
      "[Eval 16]  Return: 21.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 17] Return:  5.00 | Ep. length: 100.0\n",
      "[Eval 17]  Return:  1.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 18] Return:  4.47 | Ep. length: 100.0\n",
      "[Eval 18]  Return: 19.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 19] Return:  3.85 | Ep. length: 100.0\n",
      "[Eval 19]  Return: 20.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 20] Return:  4.05 | Ep. length: 100.0\n",
      "[Eval 20]  Return: 20.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 21] Return:  4.45 | Ep. length: 100.0\n",
      "[Eval 21]  Return: 11.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 22] Return:  4.13 | Ep. length: 100.0\n",
      "[Eval 22]  Return: 50.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 23] Return:  4.08 | Ep. length: 100.0\n",
      "[Eval 23]  Return: 19.40  | Ep. length: 100.0\n",
      "\n",
      "\t[EPS]: 0.7509908003394611\n",
      "[Train 24] Return:  3.68 | Ep. length: 100.0\n",
      "[Eval 24]  Return: 21.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 25] Return:  3.15 | Ep. length: 100.0\n",
      "[Eval 25]  Return: 20.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 26] Return:  3.19 | Ep. length: 100.0\n",
      "[Eval 26]  Return: 60.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 27] Return:  3.36 | Ep. length: 100.0\n",
      "[Eval 27]  Return: 28.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 28] Return:  3.10 | Ep. length: 100.0\n",
      "[Eval 28]  Return: 32.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 29] Return:  2.70 | Ep. length: 100.0\n",
      "[Eval 29]  Return: 28.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 30] Return:  2.77 | Ep. length: 100.0\n",
      "[Eval 30]  Return: 22.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 31] Return:  2.81 | Ep. length: 100.0\n",
      "[Eval 31]  Return: 24.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 32] Return:  3.30 | Ep. length: 100.0\n",
      "[Eval 32]  Return:  0.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 33] Return:  3.59 | Ep. length: 100.0\n",
      "[Eval 33]  Return: 12.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 34] Return:  3.46 | Ep. length: 100.0\n",
      "[Eval 34]  Return: 22.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 35] Return:  3.08 | Ep. length: 100.0\n",
      "[Eval 35]  Return:  2.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 36] Return:  3.11 | Ep. length: 100.0\n",
      "[Eval 36]  Return: 47.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 37] Return:  3.45 | Ep. length: 100.0\n",
      "[Eval 37]  Return:  6.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 38] Return:  3.85 | Ep. length: 100.0\n",
      "[Eval 38]  Return:  5.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 39] Return:  3.90 | Ep. length: 100.0\n",
      "[Eval 39]  Return: 12.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 40] Return:  4.05 | Ep. length: 100.0\n",
      "[Eval 40]  Return:  2.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 41] Return:  3.93 | Ep. length: 100.0\n",
      "[Eval 41]  Return: -2.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 42] Return:  4.02 | Ep. length: 100.0\n",
      "[Eval 42]  Return: 21.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 43] Return:  3.89 | Ep. length: 100.0\n",
      "[Eval 43]  Return: 30.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 44] Return:  3.80 | Ep. length: 100.0\n",
      "[Eval 44]  Return: 12.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 45] Return:  3.87 | Ep. length: 100.0\n",
      "[Eval 45]  Return: 24.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 46] Return:  3.83 | Ep. length: 100.0\n",
      "[Eval 46]  Return: 25.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 47] Return:  3.83 | Ep. length: 100.0\n",
      "[Eval 47]  Return: 32.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 48] Return:  3.84 | Ep. length: 100.0\n",
      "[Eval 48]  Return: 12.00  | Ep. length: 100.0\n",
      "\n",
      "\t[EPS]: 0.5959321842302231\n",
      "[Train 49] Return:  3.94 | Ep. length: 100.0\n",
      "[Eval 49]  Return: 33.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 50] Return:  4.18 | Ep. length: 100.0\n",
      "[Eval 50]  Return: 21.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 51] Return:  4.46 | Ep. length: 100.0\n",
      "[Eval 51]  Return:  5.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 52] Return:  4.60 | Ep. length: 100.0\n",
      "[Eval 52]  Return: 22.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 53] Return:  4.52 | Ep. length: 100.0\n",
      "[Eval 53]  Return: 34.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 54] Return:  4.38 | Ep. length: 100.0\n",
      "[Eval 54]  Return: 16.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 55] Return:  4.82 | Ep. length: 100.0\n",
      "[Eval 55]  Return: 24.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 56] Return:  4.72 | Ep. length: 100.0\n",
      "[Eval 56]  Return: 13.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 57] Return:  4.62 | Ep. length: 100.0\n",
      "[Eval 57]  Return: 19.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 58] Return:  4.66 | Ep. length: 100.0\n",
      "[Eval 58]  Return: 30.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 59] Return:  4.72 | Ep. length: 100.0\n",
      "[Eval 59]  Return: 42.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 60] Return:  4.64 | Ep. length: 100.0\n",
      "[Eval 60]  Return: 42.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 61] Return:  4.47 | Ep. length: 100.0\n",
      "[Eval 61]  Return: 52.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 62] Return:  4.30 | Ep. length: 100.0\n",
      "[Eval 62]  Return: 32.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 63] Return:  4.28 | Ep. length: 100.0\n",
      "[Eval 63]  Return: 40.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 64] Return:  4.23 | Ep. length: 100.0\n",
      "[Eval 64]  Return: 58.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 65] Return:  4.48 | Ep. length: 100.0\n",
      "[Eval 65]  Return: 17.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 66] Return:  4.51 | Ep. length: 100.0\n",
      "[Eval 66]  Return: 56.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 67] Return:  4.63 | Ep. length: 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-140ce0af9550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 print(f\"[Train {i_episode:d}] Return: {np.mean(returns):5.2f} \"\n\u001b[1;32m     44\u001b[0m                       f\"| Ep. length: {np.mean(episode_durations)}\")\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0meval_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_no_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5d5bd298e1e3>\u001b[0m in \u001b[0;36meval_agent\u001b[0;34m(env, model, no_episodes, show, waitkey)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mep_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/gathering_mae-0.1-py3.6.egg/gathering_mae/single_agent_wrapper.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mep_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/gathering_mae-0.1-py3.6.egg/gathering_mae/gathering_env.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m             self.el_init, _ = self.map_manager.init_elements(self.special_map,\n\u001b[1;32m    789\u001b[0m                                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                                                              self.reward_init_pos)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mel_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/gathering_mae-0.1-py3.6.egg/gathering_mae/gathering_env.py\u001b[0m in \u001b[0;36minit_elements\u001b[0;34m(self, map_, occupancy_map, init_pos)\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_coord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miselect\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mmap_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mel_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_linear_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mix_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_items\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/summer_py/lib/python3.6/site-packages/gathering_mae-0.1-py3.6.egg/gathering_mae/gathering_env.py\u001b[0m in \u001b[0;36mget_linear_pos\u001b[0;34m(x, y, width)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_linear_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#screen = env.render()\n",
    "#cv.named\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = process_obs(state)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state.to(device))\n",
    "        action = action.cpu()\n",
    "        current_screen, reward, done, _ = env.step(action.item())\n",
    "        current_screen = process_obs(current_screen)\n",
    "\n",
    "        ep_return += reward\n",
    "        steps_cnt += 1\n",
    "        \n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = current_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward.cpu())\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if i_episode > EP_START_OPTIM and steps_cnt % OPTIMIZE_FREQ == 0:\n",
    "            optimize_model()\n",
    "\n",
    "        if done:\n",
    "            returns.append(ep_return)\n",
    "            episode_durations.append(t + 1)\n",
    "            ep_return = 0\n",
    "            \n",
    "            if i_episode % print_freq == 0:\n",
    "                print(f\"[Train {i_episode:d}] Return: {np.mean(returns):5.2f} \"\n",
    "                      f\"| Ep. length: {np.mean(episode_durations)}\")\n",
    "                eval_agent(env, policy_net, no_episodes = eval_no_episodes, show = False)\n",
    "                print()\n",
    "                \n",
    "            if wait_key and i_episode % play_game_freq == 0:\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == 27:\n",
    "                    cv2.destroyAllWindows()\n",
    "                elif key == 113:  # Null action q\n",
    "                    eval_agent(env, policy_net, no_episodes = 1, show = True)\n",
    "\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
