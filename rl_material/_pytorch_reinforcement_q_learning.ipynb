{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code adapted from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) tutorial\n",
    "=====================================\n",
    "**Author**: `Adam Paszke <https://github.com/apaszke>`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gym env\n",
    "# env = gym.make('CartPole-v0').unwrapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gathering envrionment\n",
    "\n",
    "from gathering_mae.single_agent_wrapper import SingleAgentGatheringEnv\n",
    "from configs.utils import load_config\n",
    "import cv2\n",
    "\n",
    "# Get default config\n",
    "\n",
    "cfg = load_config(\"configs/static_simple.yaml\")\n",
    "\n",
    "# cfg = load_config(\"configs/default_env.yaml\")\n",
    "\n",
    "env = SingleAgentGatheringEnv(cfg)\n",
    "obs_size: torch.Size = env.observation_space\n",
    "no_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DQN algorithm\n",
    "-------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_size[0], 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.head = nn.Linear(288, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu((self.conv1(x)))\n",
    "        x = F.relu((self.conv2(x)))\n",
    "        x = F.relu((self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size: torch.Size, out_size: torch.Size):\n",
    "        super(MLP, self).__init__()\n",
    "        in_units = reduce(mul, in_size, 1)\n",
    "        hidden_size = 256\n",
    "        self.ln1 = nn.Linear(in_units, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.ln2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, out_size[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn1(self.ln1(x)))\n",
    "        x = F.relu(self.bn2(self.ln2(x)))\n",
    "        return self.head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input extraction & processing\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUQAAAEICAYAAAAncI3RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFxpJREFUeJzt3Xm0HGWdxvHvkwVCAsoiOkIiAVkElG0QVESQRXZcxgUccMRRdFwADyPuuOtxxkEcdVAE3NhkEQQGCCCbjhokEBAIYARiQoiEPSwCgd/88f6uVNq+W246lar7fM65J91ddat/VfXWU29Vd+6riMDMzGBM3QWYma0oHIhmZsmBaGaWHIhmZsmBaGaWHIhmZqkVgSjp3ZJ+3YPlfk/SZ5f1cofx/ptIul7SIkmH1VVHU0maKikkjau5jp60I0mfl3TyUv5uSNpwWdfUS5J2lHRbL99j0IYi6S7gRcAzlZd/FBEf7lVRK4qI+EDNJRwFXBkRW9dcxxIk/QiYFxGf6cGyA9goImYv62XXZQVoR43U2RYi4lfAJr18z6GeOfeLiMt6WYh1tR5wet1FjAaSxkXE4rrraBNJYyPimcHnXIFExIA/wF3Abv1MOw44q/L868AvAQFrABcAC4EH8/HkyrxXAl8GfgM8CpwPrAWcAjwC/B6YWpk/gMOAO4D7gP8ExuS0dwO/rsz7MuBS4AHgNuDt/dR/AHBtx2sfBc7Lxz8CvlyZti8wE3go694iXz8EOL8y32zgjMrzucBW/dSwP3BzLvNKYNN8/XJKr/yvuX027vK7hwCzgEW5Xd7fMf0o4B5gPvDe3IYb5rSVgW8Afwb+AnwPWCWn7QzMA44E7s1lHJLTDgWeBp7q22/9rNe3cr0fAWYAO1amjQU+Bfwpa58BTAGuzhofy2W/o3PfVtpC33rsA1yf7zMX+Hxlvqk577gB2vbHgRuBJykdhHWAsynt9k7gsMHqHqzNVdtR7q99K9PGUdrzNvn8VZS29RBwA7BzZd71gavyvS8FvgOcPMCx+z5KW3wAOA9YZ4jH04b5Pg/ntJ8N5djK9TwOuDD34WeABcDYyjxvBm7Mx9sBv811vSfXZ6Wc1q0t7Ey5Mulb1qaUY+YhyjG0f0ct3wX+N7fXdOClg+bdCANxInA7pdHumBtvck5bC/innGc14Ezg3I5AnA28FHg+cEsua7dsJD8BftixA68A1gRekvO+tzMQgUmUA+OQXM42Wdfm/dS/iNIt73vt98ABXRryNpRw2J5yYPxLbpuVgQ1yp4wBXgzMAe7O39uAckIY0+X9N84dvjswnhJgsyuN4sq+dexn+++T20/ATsDjPHdg7UlpjJvnev6UJYPkWMpBsmbun/OBr1UCcTHwxaxr71z2Gt1OFP3UdlC2gXGUYF0ATMhpHwP+QLn8EbAlsFZn2HXu234CcWfgFbntt6CE+5uGEYgzKWG8Si5jBnA0sFLuuzuAPQaqm0HaHEu2o6OBUzr24a35eF3g/tzeY7Jd3A+sndN/CxxDaXOvo7TdroEI7JI1bJPzfxu4eojH02nAp7OGCcBrh3Js5Xo+DOxQ+d0/AbtX3vdM4BP5+B8pJ4Bxua9mAUd028+VfT0vH4+nHCufyn21S26PTSq1PEAJ3XGUjtbpyyoQH6Uc8H0/76tM3y7feA5w4ADL2Qp4sCMQP115/l/ARZXn+wEzOzbOnpXnHwR+2SUQ3wH8quO9vw98rp+6TgaOzscb5Uad2KUhHwd8qeN3bwN2ysdzs4EcABwPXEM5mx5C9ji7vPdnWbInOQa4m+wVMEggdlneucDh+fgkMuDy+YZ9DYxyMD9G5YwJvBq4s9LwnqASJJSTwauGGohdansQ2LKy3d7Yz3zDCsQuv38s8M18PJXBA/E9lefbA3/umOeT5Im5v7oHa3Md7WjDjjZ2SqX9fRz4acdyplFOvi+hnKQmVaadSv+BeCLwH5Xnq1J69lOHcDz9JNvw5KVYz590TP8ycFI+Xi3b3Xr91HwEcM4AbWFnngvEHSkn2TGV6aeRVwhZywmVaXuTJ56Bfob6KfObImL1ys8P+iZExDWUs6iAM/pelzRR0vclzZH0CKULvLqksZXl/qXy+Ikuz1ftqGNu5fEcyuVNp/WA7SU91PcD/DPwD/2s26nAgfn4nZRe7OP9LPfIjuVOqdRwFWWHvS4fX0npte2Uz7tZJ9cDgIh4Ntdx3X7mX4KkvST9TtIDWc/ewAsqy65ur+rjtSm9xhmVdbk4X+9zfyx5T+1x/n5/DFTbkZJmSXo4l//8Sm1TKD2HEZO0vaQrJC2U9DDwgcr7DEV1u6wHrNOxjz9F+VBxoLqH3OaifEAwC9hP0kTKLZNTK8t5W8dyXku56liH0qF4rLK4OfSvs209SultVttWf8fTUZTj+RpJN0t6zzDWs7pMct3eImll4C3AdRExB0DSxpIukLQgM+KrDH3frQPMzWOmug7V9VtQeTyk9jviryNI+hClSz6fsiG/lpOOpFxabB8RCyRtRbnXoxG83RTKvQIoZ8z5XeaZC1wVEbsPcZmXAC/I+g6k3EPsZi7wlYj4Sj/Tr6L0aten7Ni+xvJqyr2RbuZTLvcAkCTKOt49WNHZwM4G3gX8IiKelnQuz23fe4DJlV+ZUnl8H+WEs3lEDPpeXcQgte1I6e3sCtwcEc9KerBS21zKpf5NQ3ivxyjh3bfszpA5lbJ994qIv0o6luEFYnVd5lJ6yRv1M29/dQ+3zZ1GaWtjgFviuU/U51J6iO/r/AVJ6wFrSJpUCcWX0P++mE8JsL7fn0S5vK/u767HU0QsoNx/RNJrgcskXT3E9Vyinoi4RdIcYC9Kh+PUyuTjKJlwYEQsknQE8NYBlt25flMkjamEYt+l/1Ib0fcQJW1M6RIfBBwMHJXBAqV7/ATwkKQ1gc+N5L3SxyStIWkKcDjwsy7zXABsLOlgSePz55WSNu22wOwFnUW5qbwm5YZxNz8APpA9EkmaJGkfSavl9KuA11M+mJgH/IpyH28tyk7v5gxgH0m7ShpPOYk8SbmpPpiVKCeihcBiSXsBb+hY9iGSNs2eyNGVdX421+ebkl4IIGldSXsM4X2h9OQ3GGD6apTLu4XAOElHA8+rTD8B+JKkjXJbbiFprX6WfQOwuaStJE0APt/lvR7IMNyOctAtrWuARyR9XNIqksZKermkVw5S97DaHOWbA28A/o0lA+JkSs9xj3zvCZJ2ljQ5e1XXAl+QtFIG1X4DrMuplP2/VZ48vwpMj4i7KvN0PZ4kvU1S38n0QUrIPbMU61mt5TDK1dOZlddXo3wY9qikl+X2qBqonU2nnCyPyjp2pmyPEX0rY6iBeL6kRys/56h82fVk4OsRcUNE/JFyefHT3AHHUm5U3wf8jnJJNlK/oNz0nkn59OjEzhkiYhGlsR1AOYssoHz6vfIAyz2V8mHOmdHPVy8i4lrKWfM7lEYym3J/q2/67ZR7rb/K549QbiX8X/Tz1YOIuI1yMvk2ZTvtR/mK01MD1Fpdz8MowfcgJQjOq0y/CPhvyo3z2ZQb8lACF0oPbjbwu7xcuYyhf8frRGCzvGw6t8v0acBFlLP1HMon5dVLqWOy7ksoB8SJlLYCJfB+nMt+e27XL2Z9fwQ6v4D/QeCLkhZRQv8MllLup/0o97vvpOyTEyiX+/3WPdw2FxH3UPbHa6ic1CNiLvBGynG0kLLNPsZzx+k7Kfc5H6B0MH4ywLr8knKP+mzK1cJLs76q/o6nVwLTJT1KaVOHR8SdS3lsQekR7wxcHhH3VV7/91ynRZQTdGcH5/NU2kLH+j1Fud2wF2U//Q/wroi4dZBaBqS84bjCUwu/sLs85Vn8JmDl/kLfbLRrxX/ds+4kvTkvr9agnMnPdxia9c+B2G7vp1x6/YlyD6jzHo2ZVTTmktnMrNfcQzQzS7X+WaReWUkrxwQm9Wz5G2/R7Xvby87tN04cfCbrqdU2e3bwmUZo0S3N7o8s4sH7ImLtwedsjlYG4gQmsb127dnyp02b2bNlA+yxzlaDz2Q9tdPPnuj5e1y1xSqDz7QCuyzOGuh/yjRSs09RZmbLkAPRzCw5EM3MkgPRzCw5EM3MkgPRzCw5EM3MUmMCUdKekm6TNFvSJ+qux8zapxGBqDLswHcpf/tsM+BASZvVW5WZtU0jApEykNXsiLgj/zDk6ZQ/pGlmtsw0JRDXZcm/uDyPjoGYJB0q6VpJ1z79tz8KbWY2dE0JxG4DU3UOZnN8RGwbEduOH/QvmpuZ/b2mBOI8lhw1bjLdR9wzM1tqTQnE3wMbSVpf0kqUQW7OG+R3zMyGpRF//isiFkv6MGU0t7HASRFx8yC/ZmY2LI0IRICIuBC4sO46zKy9mnLJbGbWcw5EM7PkQDQzSw5EM7PkQDQzSw5EM7PUmK/dDMfGWzze06FCez1M6LT5vR3mFHq/Djvd2NthPJs+hKetmNxDNDNLDkQzs+RANDNLDkQzs+RANDNLDkQzs+RANDNLDkQzs9SIQJR0kqR7Jd1Udy1m1l6NCETgR8CedRdhZu3WiECMiKuBB+quw8zarRGBaGa2PLQmEKsD1S+8/5m6yzGzBmpNIFYHql97rbF1l2NmDdSaQDQzG6lGBKKk04DfAptImifpX+uuyczapxF/IDYiDqy7BjNrv0b0EM3MlgcHoplZciCamSUHoplZciCamSUHoplZasTXbobr9hsn9nTc4V6Pm9zrMZOXh16Pm+xxnwfX62102St6uvhauIdoZpYciGZmyYFoZpYciGZmyYFoZpYciGZmyYFoZpYciGZmyYFoZpYaEYiSpki6QtIsSTdLOrzumsysfZryX/cWA0dGxHWSVgNmSLo0Im6puzAza49G9BAj4p6IuC4fLwJmAevWW5WZtU1Teoh/I2kqsDUwveP1Q4FDASYwcbnXZWbN14geYh9JqwJnA0dExCPVadVxmcezcj0FmlmjNSYQJY2nhOEpEfHzuusxs/ZpRCBKEnAiMCsijqm7HjNrp0YEIrADcDCwi6SZ+bN33UWZWbs04kOViPg1oLrrMLN2a0oP0cys5xyIZmbJgWhmlhyIZmbJgWhmlhyIZmapEV+7WdH0eiD5afNn9nT50Px16HX9vR7kHeCqLVbp+XvY8LiHaGaWHIhmZsmBaGaWHIhmZsmBaGaWHIhmZsmBaGaWHIhmZqkRgShpgqRrJN2Q4zJ/oe6azKx9mvI/VZ4EdomIR3NslV9Luigifld3YWbWHo0IxIgI4NF8Oj5/or6KzKyNGnHJDCBprKSZwL3ApRHxd+MyS7pW0rVP82Q9RZpZozUmECPimYjYCpgMbCfp5R3TPS6zmY1IYwKxT0Q8BFwJ7FlzKWbWMo0IRElrS1o9H68C7AbcWm9VZtY2jfhQBXgx8GNJYykhfkZEXFBzTWbWMo0IxIi4Edi67jrMrN0acclsZrY8OBDNzJID0cwsORDNzJID0cwsORDNzFIjvnYzXBtv8TjTpvVuXOBejwn81fs26enyAW7/3nY9Xf7m335NT5c/md/0dPkeM3l0cg/RzCw5EM3MkgPRzCw5EM3MkgPRzCw5EM3MkgPRzCw5EM3MUqMCMQeaul6S/zismS1zjQpE4HBgVt1FmFk7NSYQJU0G9gFOqLsWM2unxgQicCxwFPBst4nVcZkX3v/M8q3MzFqhEYEoaV/g3oiY0d881XGZ115r7HKszszaohGBCOwA7C/pLuB0YBdJJ9dbkpm1TSMCMSI+GRGTI2IqcABweUQcVHNZZtYyjQhEM7PloXF/IDYirgSurLkMM2sh9xDNzJID0cwsORDNzJID0cwsORDNzJID0cwsKSLqrmGZe57WjO21a8+Wv9ONT/Rs2dCOMYGnze/duNjQ+7Gxe72Pofn7+bI4a0ZEbFt3HcuSe4hmZsmBaGaWHIhmZsmBaGaWHIhmZsmBaGaWHIhmZsmBaGaWGvP3EHP4gEXAM8Ditn0h1Mzq15hATK+PiPvqLsLM2smXzGZmqUmBGMAlkmZIOrRzYnVc5qd5sobyzKzpmnTJvENEzJf0QuBSSbdGxNV9EyPieOB4KH/coa4izay5GtNDjIj5+e+9wDnAdvVWZGZt04hAlDRJ0mp9j4E3ADfVW5WZtU1TLplfBJwjCUrNp0bExfWWZGZt04hAjIg7gC3rrsPM2q0Rl8xmZsuDA9HMLDkQzcySA9HMLDkQzcySA9HMLDXiazcrml6Pp9vrMY2h9+MaN13Tx0y2peMeoplZciCamSUHoplZciCamSUHoplZciCamSUHoplZciCamaXGBKKk1SWdJelWSbMkvbrumsysXZr0P1W+BVwcEW+VtBIwse6CzKxdGhGIkp4HvA54N0BEPAU8VWdNZtY+Tblk3gBYCPxQ0vWSTsjBpv7G4zKb2Ug1JRDHAdsAx0XE1sBjwCeqM0TE8RGxbURsO56V66jRzBquKYE4D5gXEdPz+VmUgDQzW2YaEYgRsQCYK2mTfGlX4JYaSzKzFmrEhyrpI8Ap+QnzHcAhNddjZi3TmECMiJnAtnXXYWbt1YhLZjOz5cGBaGaWHIhmZsmBaGaWHIhmZsmBaGaWHIhmZqkx30McTdowiHwb1sFGH/cQzcySA9HMLDkQzcySA9HMLDkQzcySA9HMLDkQzcxSIwJR0iaSZlZ+HpF0RN11mVm7NOKL2RFxG7AVgKSxwN3AObUWZWat04geYoddgT9FxJy6CzGzdmliIB4AnFZ3EWbWPo0KxBxgan/gzC7TPFC9mY1IowIR2Au4LiL+0jnBA9Wb2Ug1LRAPxJfLZtYjjQlESROB3YGf112LmbVTI752AxARjwNr1V2HmbVXY3qIZma95kA0M0sORDOz5EA0M0sORDOz5EA0M0sORDOz5EA0M0sORDOz5EA0M0sORDOz5EA0M0sORDOz5EA0M0sORDOz5EA0M0uNCURJH5V0s6SbJJ0maULdNZlZuzQiECWtCxwGbBsRLwfGUoYjNTNbZhoRiGkcsIqkccBEYH7N9ZhZyzQiECPibuAbwJ+Be4CHI+KS6jwel9nMRqoRgShpDeCNwPrAOsAkSQdV5/G4zGY2Uo0IRGA34M6IWBgRT1OGIn1NzTWZWcs0JRD/DLxK0kRJAnYFZtVck5m1TCMCMSKmA2cB1wF/oNR9fK1FmVnrNGmg+s8Bn6u7DjNrr0b0EM3MlgcHoplZciCamSUHoplZciCamSUHoplZUkTUXcMyJ2khMGcYv/IC4L4elbO8NH0dXH/9hrsO60XE2r0qpg6tDMThknRtRGxbdx0j0fR1cP31a8M6jJQvmc3MkgPRzCw5EIs2/L/opq+D669fG9ZhRHwP0cwsuYdoZpYciGZmadQHoqQ9Jd0mabakT9Rdz3BImiLpCkmzcojWw+uuaWlIGivpekkX1F3L0pC0uqSzJN2a++LVddc0HB7i9zmjOhAljQW+C+wFbAYcKGmzeqsalsXAkRGxKfAq4EMNq7/P4TT7L6B/C7g4Il4GbEmD1sVD/C5pVAcisB0wOyLuiIingNMpg1k1QkTcExHX5eNFlANx3XqrGh5Jk4F9gBPqrmVpSHoe8DrgRICIeCoiHqq3qmHzEL9ptAfiusDcyvN5NCxQ+kiaCmwNTK+3kmE7FjgKeLbuQpbSBsBC4Id52X+CpEl1FzVUQxnidzQZ7YGoLq817ntIklYFzgaOiIhH6q5nqCTtC9wbETPqrmUExgHbAMdFxNbAY0Bj7kUPZYjf0WS0B+I8YErl+WQadrkgaTwlDE+JiJ/XXc8w7QDsL+kuyu2KXSSdXG9JwzYPmJcDoUEZDG2bGusZLg/xWzHaA/H3wEaS1pe0EuVm8nk11zRkOSTricCsiDim7nqGKyI+GRGTI2IqZdtfHhGN6p1ExAJgrqRN8qVdgVtqLGm4PMRvRWNG3euFiFgs6cPANMqnaydFxM01lzUcOwAHA3+QNDNf+1REXFhjTaPRR4BT8qR6B3BIzfUMWURMl9Q3xO9i4HpG8X/h83/dMzNLo/2S2czsbxyIZmbJgWhmlhyIZmbJgWhmlhyIZmbJgWhmlv4fLGQPCA7ep6cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize env capture\n",
    "def convert_obs_to_screen(obs):\n",
    "    obs = obs.clone()\n",
    "    obs.mul_(255).int()\n",
    "    obs = obs.numpy().astype(np.uint8)\n",
    "    return obs\n",
    "\n",
    "def process_obs(obs):\n",
    "    obs = obs.unsqueeze(0).unsqueeze(0)\n",
    "    obs.add_(-0.6588499999999999).mul_(2.)\n",
    "    return obs\n",
    "\n",
    "obs = env.reset()\n",
    "screen = env.render()\n",
    "view_obs = convert_obs_to_screen(obs)\n",
    "\n",
    "# Plot full map + partial \n",
    "plt.figure()\n",
    "plt.imshow(screen,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of game full map & agent observation')\n",
    "plt.show()\n",
    "\n",
    "# Plot actual observation \n",
    "plt.figure()\n",
    "plt.imshow(view_obs,\n",
    "           interpolation='none')\n",
    "plt.title('Example view of agent actual received observation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation & visualization \n",
    "\n",
    "def eval_agent(env, model, no_episodes = 5, show = True, waitkey=0):\n",
    "\n",
    "    model.eval()\n",
    "    done = True\n",
    "    env_step = 0\n",
    "    \n",
    "    eval_returns = []\n",
    "    eval_lengths = []\n",
    "    for ep in range(no_episodes):\n",
    "        ep_return = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        env.render(imshow=show)\n",
    "\n",
    "        for t in count():                \n",
    "\n",
    "            with torch.no_grad():\n",
    "                state = process_obs(state).to(device)\n",
    "                action = model(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "            if show:\n",
    "                key = cv2.waitKey(waitkey) & 0xFF\n",
    "\n",
    "                # if the 'ESC' key is pressed, Quit\n",
    "                if key == 27:\n",
    "                    show = False\n",
    "\n",
    "            state, r, done, _ = env.step(action)\n",
    "            ep_return += r\n",
    "\n",
    "            env.render(imshow=show)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eval_returns.append(ep_return)\n",
    "        eval_lengths = [t + 1]\n",
    "        \n",
    "    print(f\"[Eval {i_episode:d}]  Return: {np.mean(eval_returns):5.2f}\",\n",
    "      \" | Ep. length:\", np.mean(eval_lengths))\n",
    "\n",
    "\n",
    "# Report info\n",
    "wait_key = False\n",
    "print_freq = 1\n",
    "\n",
    "\n",
    "# screen = env.render()\n",
    "# cv2.namedWindow(\"Waikey\")\n",
    "# wait_key = True\n",
    "# play_game_freq = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.4\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 4\n",
    "EP_START_OPTIM = 2\n",
    "OPTIMIZE_FREQ = 4\n",
    "\n",
    "policy_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "target_net = DQN(in_size=obs_size, out_size=torch.Size([no_actions])).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "steps_done = 0\n",
    "steps_cnt = 0\n",
    "episode_durations = []\n",
    "returns = []\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if steps_done % (EPS_DECAY // 4) == 0:\n",
    "        print(f\"\\-- EPS: {eps_threshold}\")\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(no_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "^^^^^^^^^^^^^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    policy_net.train()\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).to(device)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]).to(device)\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500\n",
    "eval_no_episodes = 5\n",
    "ep_return = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train 0] Return:  3.00 | Ep. length: 100.0\n",
      "[Eval 0]  Return: 19.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 1] Return: -7.50 | Ep. length: 100.0\n",
      "[Eval 1]  Return: -10.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 2] Return: -7.00 | Ep. length: 100.0\n",
      "[Eval 2]  Return: -9.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 3] Return: -4.00 | Ep. length: 100.0\n",
      "[Eval 3]  Return:  1.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 4] Return: -5.40 | Ep. length: 100.0\n",
      "[Eval 4]  Return: 20.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 5] Return: -4.50 | Ep. length: 100.0\n",
      "[Eval 5]  Return: -1.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 6] Return: -2.57 | Ep. length: 100.0\n",
      "[Eval 6]  Return: -9.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 7] Return: -1.88 | Ep. length: 100.0\n",
      "[Eval 7]  Return: -9.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 8] Return: -1.22 | Ep. length: 100.0\n",
      "[Eval 8]  Return: 11.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 9] Return: -0.60 | Ep. length: 100.0\n",
      "[Eval 9]  Return: 21.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 10] Return:  0.00 | Ep. length: 100.0\n",
      "[Eval 10]  Return: 11.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 11] Return:  1.33 | Ep. length: 100.0\n",
      "[Eval 11]  Return: 31.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 12] Return:  1.38 | Ep. length: 100.0\n",
      "[Eval 12]  Return:  2.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 13] Return:  1.57 | Ep. length: 100.0\n",
      "[Eval 13]  Return:  3.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 14] Return:  2.00 | Ep. length: 100.0\n",
      "[Eval 14]  Return:  5.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 15] Return:  1.19 | Ep. length: 100.0\n",
      "[Eval 15]  Return: 33.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 16] Return:  1.35 | Ep. length: 100.0\n",
      "[Eval 16]  Return:  3.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 17] Return:  2.00 | Ep. length: 100.0\n",
      "[Eval 17]  Return: 63.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 18] Return:  2.53 | Ep. length: 100.0\n",
      "[Eval 18]  Return: 34.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 19] Return:  2.65 | Ep. length: 100.0\n",
      "[Eval 19]  Return: 20.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 20] Return:  2.57 | Ep. length: 100.0\n",
      "[Eval 20]  Return:  1.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 21] Return:  2.36 | Ep. length: 100.0\n",
      "[Eval 21]  Return: 21.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 22] Return:  3.35 | Ep. length: 100.0\n",
      "[Eval 22]  Return: 22.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 23] Return:  2.92 | Ep. length: 100.0\n",
      "[Eval 23]  Return: 21.60  | Ep. length: 100.0\n",
      "\n",
      "\\-- EPS: 0.7509908003394611\n",
      "[Train 24] Return:  2.80 | Ep. length: 100.0\n",
      "[Eval 24]  Return: 43.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 25] Return:  3.35 | Ep. length: 100.0\n",
      "[Eval 25]  Return: 13.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 26] Return:  3.63 | Ep. length: 100.0\n",
      "[Eval 26]  Return:  8.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 27] Return:  3.54 | Ep. length: 100.0\n",
      "[Eval 27]  Return: 11.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 28] Return:  3.48 | Ep. length: 100.0\n",
      "[Eval 28]  Return:  1.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 29] Return:  3.87 | Ep. length: 100.0\n",
      "[Eval 29]  Return: 51.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 30] Return:  3.84 | Ep. length: 100.0\n",
      "[Eval 30]  Return: 28.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 31] Return:  3.59 | Ep. length: 100.0\n",
      "[Eval 31]  Return: 25.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 32] Return:  3.21 | Ep. length: 100.0\n",
      "[Eval 32]  Return: 17.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 33] Return:  3.03 | Ep. length: 100.0\n",
      "[Eval 33]  Return:  3.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 34] Return:  3.14 | Ep. length: 100.0\n",
      "[Eval 34]  Return: 29.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 35] Return:  2.61 | Ep. length: 100.0\n",
      "[Eval 35]  Return: -7.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 36] Return:  2.65 | Ep. length: 100.0\n",
      "[Eval 36]  Return:  3.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 37] Return:  2.76 | Ep. length: 100.0\n",
      "[Eval 37]  Return: 24.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 38] Return:  2.87 | Ep. length: 100.0\n",
      "[Eval 38]  Return: 34.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 39] Return:  2.77 | Ep. length: 100.0\n",
      "[Eval 39]  Return: -0.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 40] Return:  2.80 | Ep. length: 100.0\n",
      "[Eval 40]  Return:  3.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 41] Return:  2.95 | Ep. length: 100.0\n",
      "[Eval 41]  Return: 36.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 42] Return:  3.14 | Ep. length: 100.0\n",
      "[Eval 42]  Return: 20.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 43] Return:  3.27 | Ep. length: 100.0\n",
      "[Eval 43]  Return: 12.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 44] Return:  3.40 | Ep. length: 100.0\n",
      "[Eval 44]  Return: 10.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 45] Return:  3.87 | Ep. length: 100.0\n",
      "[Eval 45]  Return: 35.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 46] Return:  4.09 | Ep. length: 100.0\n",
      "[Eval 46]  Return: 40.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 47] Return:  4.15 | Ep. length: 100.0\n",
      "[Eval 47]  Return: 59.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 48] Return:  4.39 | Ep. length: 100.0\n",
      "[Eval 48]  Return:  9.60  | Ep. length: 100.0\n",
      "\n",
      "\\-- EPS: 0.5959321842302231\n",
      "[Train 49] Return:  4.18 | Ep. length: 100.0\n",
      "[Eval 49]  Return: 21.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 50] Return:  4.29 | Ep. length: 100.0\n",
      "[Eval 50]  Return: 26.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 51] Return:  4.33 | Ep. length: 100.0\n",
      "[Eval 51]  Return: 24.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 52] Return:  4.30 | Ep. length: 100.0\n",
      "[Eval 52]  Return: 23.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 53] Return:  4.04 | Ep. length: 100.0\n",
      "[Eval 53]  Return: 70.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 54] Return:  3.95 | Ep. length: 100.0\n",
      "[Eval 54]  Return: 22.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 55] Return:  3.89 | Ep. length: 100.0\n",
      "[Eval 55]  Return: 61.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 56] Return:  3.77 | Ep. length: 100.0\n",
      "[Eval 56]  Return: 30.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 57] Return:  4.03 | Ep. length: 100.0\n",
      "[Eval 57]  Return:  8.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 58] Return:  4.25 | Ep. length: 100.0\n",
      "[Eval 58]  Return: 41.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 59] Return:  4.30 | Ep. length: 100.0\n",
      "[Eval 59]  Return:  7.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 60] Return:  4.64 | Ep. length: 100.0\n",
      "[Eval 60]  Return: 27.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 61] Return:  4.92 | Ep. length: 100.0\n",
      "[Eval 61]  Return: 20.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 62] Return:  5.30 | Ep. length: 100.0\n",
      "[Eval 62]  Return: 47.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 63] Return:  5.31 | Ep. length: 100.0\n",
      "[Eval 63]  Return: 40.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 64] Return:  5.49 | Ep. length: 100.0\n",
      "[Eval 64]  Return: 15.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 65] Return:  5.62 | Ep. length: 100.0\n",
      "[Eval 65]  Return: 31.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 66] Return:  5.67 | Ep. length: 100.0\n",
      "[Eval 66]  Return: 59.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 67] Return:  5.81 | Ep. length: 100.0\n",
      "[Eval 67]  Return: 27.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 68] Return:  5.99 | Ep. length: 100.0\n",
      "[Eval 68]  Return: 80.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 69] Return:  6.06 | Ep. length: 100.0\n",
      "[Eval 69]  Return: 38.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 70] Return:  5.97 | Ep. length: 100.0\n",
      "[Eval 70]  Return: 18.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 71] Return:  5.94 | Ep. length: 100.0\n",
      "[Eval 71]  Return: 41.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 72] Return:  5.96 | Ep. length: 100.0\n",
      "[Eval 72]  Return: 24.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 73] Return:  6.23 | Ep. length: 100.0\n",
      "[Eval 73]  Return: 51.00  | Ep. length: 100.0\n",
      "\n",
      "\\-- EPS: 0.47517241258238024\n",
      "[Train 74] Return:  6.49 | Ep. length: 100.0\n",
      "[Eval 74]  Return: 50.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 75] Return:  6.45 | Ep. length: 100.0\n",
      "[Eval 75]  Return: 19.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 76] Return:  6.73 | Ep. length: 100.0\n",
      "[Eval 76]  Return: 32.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 77] Return:  6.72 | Ep. length: 100.0\n",
      "[Eval 77]  Return: 42.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 78] Return:  6.66 | Ep. length: 100.0\n",
      "[Eval 78]  Return: 50.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 79] Return:  6.65 | Ep. length: 100.0\n",
      "[Eval 79]  Return: 37.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 80] Return:  6.89 | Ep. length: 100.0\n",
      "[Eval 80]  Return: 60.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 81] Return:  7.00 | Ep. length: 100.0\n",
      "[Eval 81]  Return: 56.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 82] Return:  7.27 | Ep. length: 100.0\n",
      "[Eval 82]  Return: 44.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 83] Return:  7.18 | Ep. length: 100.0\n",
      "[Eval 83]  Return: 60.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 84] Return:  7.52 | Ep. length: 100.0\n",
      "[Eval 84]  Return: 41.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 85] Return:  7.49 | Ep. length: 100.0\n",
      "[Eval 85]  Return: 76.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 86] Return:  7.71 | Ep. length: 100.0\n",
      "[Eval 86]  Return: 50.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 87] Return:  7.73 | Ep. length: 100.0\n",
      "[Eval 87]  Return: 65.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 88] Return:  7.76 | Ep. length: 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval 88]  Return: 79.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 89] Return:  7.78 | Ep. length: 100.0\n",
      "[Eval 89]  Return: 36.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 90] Return:  7.84 | Ep. length: 100.0\n",
      "[Eval 90]  Return: 42.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 91] Return:  7.97 | Ep. length: 100.0\n",
      "[Eval 91]  Return: 41.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 92] Return:  8.09 | Ep. length: 100.0\n",
      "[Eval 92]  Return: 35.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 93] Return:  8.10 | Ep. length: 100.0\n",
      "[Eval 93]  Return: 41.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 94] Return:  8.27 | Ep. length: 100.0\n",
      "[Eval 94]  Return: 55.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 95] Return:  8.29 | Ep. length: 100.0\n",
      "[Eval 95]  Return: 33.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 96] Return:  8.20 | Ep. length: 100.0\n",
      "[Eval 96]  Return: 56.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 97] Return:  8.36 | Ep. length: 100.0\n",
      "[Eval 97]  Return: 13.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 98] Return:  8.27 | Ep. length: 100.0\n",
      "[Eval 98]  Return: 44.00  | Ep. length: 100.0\n",
      "\n",
      "\\-- EPS: 0.38112460785951613\n",
      "[Train 99] Return:  8.58 | Ep. length: 100.0\n",
      "[Eval 99]  Return: 62.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 100] Return:  8.57 | Ep. length: 100.0\n",
      "[Eval 100]  Return: 36.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 101] Return:  8.57 | Ep. length: 100.0\n",
      "[Eval 101]  Return: 78.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 102] Return:  8.64 | Ep. length: 100.0\n",
      "[Eval 102]  Return: 13.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 103] Return:  8.75 | Ep. length: 100.0\n",
      "[Eval 103]  Return: 24.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 104] Return:  8.56 | Ep. length: 100.0\n",
      "[Eval 104]  Return: 42.80  | Ep. length: 100.0\n",
      "\n",
      "[Train 105] Return:  8.60 | Ep. length: 100.0\n",
      "[Eval 105]  Return: 44.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 106] Return:  8.66 | Ep. length: 100.0\n",
      "[Eval 106]  Return: 22.20  | Ep. length: 100.0\n",
      "\n",
      "[Train 107] Return:  8.76 | Ep. length: 100.0\n",
      "[Eval 107]  Return: 33.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 108] Return:  8.82 | Ep. length: 100.0\n",
      "[Eval 108]  Return: 31.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 109] Return:  8.71 | Ep. length: 100.0\n",
      "[Eval 109]  Return: 14.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 110] Return:  8.75 | Ep. length: 100.0\n",
      "[Eval 110]  Return: 13.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 111] Return:  8.64 | Ep. length: 100.0\n",
      "[Eval 111]  Return: 24.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 112] Return:  8.89 | Ep. length: 100.0\n",
      "[Eval 112]  Return: 31.40  | Ep. length: 100.0\n",
      "\n",
      "[Train 113] Return:  8.83 | Ep. length: 100.0\n",
      "[Eval 113]  Return: 62.00  | Ep. length: 100.0\n",
      "\n",
      "[Train 114] Return:  9.13 | Ep. length: 100.0\n",
      "[Eval 114]  Return: 36.60  | Ep. length: 100.0\n",
      "\n",
      "[Train 115] Return:  9.35 | Ep. length: 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-140ce0af9550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 print(f\"[Train {i_episode:d}] Return: {np.mean(returns):5.2f} \"\n\u001b[1;32m     44\u001b[0m                       f\"| Ep. length: {np.mean(episode_durations)}\")\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0meval_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_no_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5d5bd298e1e3>\u001b[0m in \u001b[0;36meval_agent\u001b[0;34m(env, model, no_episodes, show, waitkey)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#screen = env.render()\n",
    "#cv.named\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = process_obs(state)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state.to(device))\n",
    "        action = action.cpu()\n",
    "        current_screen, reward, done, _ = env.step(action.item())\n",
    "        current_screen = process_obs(current_screen)\n",
    "\n",
    "        ep_return += reward\n",
    "        steps_cnt += 1\n",
    "        \n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = current_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward.cpu())\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if i_episode > EP_START_OPTIM and steps_cnt % OPTIMIZE_FREQ == 0:\n",
    "            optimize_model()\n",
    "\n",
    "        if done:\n",
    "            returns.append(ep_return)\n",
    "            episode_durations.append(t + 1)\n",
    "            ep_return = 0\n",
    "            \n",
    "            if i_episode % print_freq == 0:\n",
    "                print(f\"[Train {i_episode:d}] Return: {np.mean(returns):5.2f} \"\n",
    "                      f\"| Ep. length: {np.mean(episode_durations)}\")\n",
    "                eval_agent(env, policy_net, no_episodes = eval_no_episodes, show = False)\n",
    "                print()\n",
    "                \n",
    "            if wait_key and i_episode % play_game_freq == 0:\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == 27:\n",
    "                    cv2.destroyAllWindows()\n",
    "                elif key == 113:  # Null action q\n",
    "                    eval_agent(env, policy_net, no_episodes = 1, show = True)\n",
    "\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
